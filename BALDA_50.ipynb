{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronraiftorres/BSCS1A-TORRES/blob/main/BALDA_50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25d9eb39",
      "metadata": {
        "id": "25d9eb39"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from IPython.display import IFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c22dbce2",
      "metadata": {
        "id": "c22dbce2",
        "outputId": "0e53e005-75a2-4ead-86ba-947d2661f9a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'hotspots.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cbccc9b14aad>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load each CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhotspots_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hotspots.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msst_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sst.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mssh_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ssh.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mchl_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chl.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hotspots.csv'"
          ]
        }
      ],
      "source": [
        "# Load each CSV file\n",
        "hotspots_data = pd.read_csv('hotspots.csv')\n",
        "sst_data = pd.read_csv('sst.csv')\n",
        "ssh_data = pd.read_csv('ssh.csv')\n",
        "chl_data = pd.read_csv('chl.csv')\n",
        "\n",
        "# Convert 'date' column to datetime type\n",
        "sst_data['date'] = pd.to_datetime(sst_data['date'])\n",
        "ssh_data['date'] = pd.to_datetime(ssh_data['date'])\n",
        "chl_data['date'] = pd.to_datetime(chl_data['date'])\n",
        "hotspots_data['date'] = pd.to_datetime(hotspots_data['date'])\n",
        "\n",
        "# Set 'date' column as index\n",
        "sst_data.set_index('date', inplace=True)\n",
        "ssh_data.set_index('date', inplace=True)\n",
        "chl_data.set_index('date', inplace=True)\n",
        "hotspots_data.set_index('date', inplace=True)\n",
        "\n",
        "# Resample each dataset to monthly frequency and interpolate\n",
        "sst_resampled = sst_data.resample('M').mean().interpolate(method='time')\n",
        "ssh_resampled = ssh_data.resample('M').mean().interpolate(method='time')\n",
        "chl_resampled = chl_data.resample('M').mean().interpolate(method='time')\n",
        "hotspots_resampled = hotspots_data.resample('M').mean().interpolate(method='time')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2305052",
      "metadata": {
        "id": "f2305052"
      },
      "outputs": [],
      "source": [
        "# Merge the resampled datasets\n",
        "data_resampled = pd.concat([sst_resampled, ssh_resampled, chl_resampled, hotspots_resampled], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36f5dfc8",
      "metadata": {
        "id": "36f5dfc8"
      },
      "outputs": [],
      "source": [
        "# Check for NaN values after merging\n",
        "nan_counts = data_resampled.isna().sum()\n",
        "print(\"NaN counts after merging:\")\n",
        "print(nan_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd3e6c17",
      "metadata": {
        "id": "fd3e6c17"
      },
      "outputs": [],
      "source": [
        "# Fill NaNs with backward fill method\n",
        "data_resampled.fillna(method='bfill', inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dad8416",
      "metadata": {
        "id": "5dad8416"
      },
      "outputs": [],
      "source": [
        "# Check for NaN values after merging\n",
        "nan_counts = data_resampled.isna().sum()\n",
        "print(\"NaN counts after merging:\")\n",
        "print(nan_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "122e73f2",
      "metadata": {
        "id": "122e73f2"
      },
      "outputs": [],
      "source": [
        "# Normalization\n",
        "feature_scaler = MinMaxScaler()\n",
        "target_scaler = MinMaxScaler()\n",
        "\n",
        "target_column = 'squid_abundance_per_kgs'  # Ensure this matches exactly with the column name in your data\n",
        "\n",
        "# Check if the target column exists\n",
        "if target_column not in data_resampled.columns:\n",
        "    raise KeyError(f\"The target column '{target_column}' does not exist in the data.\")\n",
        "\n",
        "# Split the data into features and target\n",
        "X = data_resampled.drop(columns=[target_column])\n",
        "y = data_resampled[target_column]\n",
        "\n",
        "# Normalize features and target\n",
        "X_normalized = pd.DataFrame(feature_scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
        "y_normalized = pd.DataFrame(target_scaler.fit_transform(y.values.reshape(-1, 1)), columns=[target_column], index=y.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c057d029",
      "metadata": {
        "id": "c057d029"
      },
      "outputs": [],
      "source": [
        "# Function to create sequences\n",
        "def create_sequences(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X.iloc[i:(i + time_steps)].values)\n",
        "        ys.append(y.iloc[i + time_steps].values)\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "time_steps = 9  # e.g., using past 9 months to predict the next month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b62afda6",
      "metadata": {
        "id": "b62afda6"
      },
      "outputs": [],
      "source": [
        "# Cross-validation\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "train_mse_scores = []\n",
        "test_mse_scores = []\n",
        "y_train_pred_all = []\n",
        "y_train_actual_all = []\n",
        "y_test_pred_all = []\n",
        "y_test_actual_all = []\n",
        "\n",
        "for train_index, test_index in tscv.split(X_normalized):\n",
        "    X_train, X_test = X_normalized.iloc[train_index], X_normalized.iloc[test_index]\n",
        "    y_train, y_test = y_normalized.iloc[train_index], y_normalized.iloc[test_index]\n",
        "    coordinates_train, coordinates_test = coordinates.iloc[train_index], coordinates.iloc[test_index]\n",
        "\n",
        "    X_train_lstm, y_train_lstm = create_sequences(X_train, y_train, time_steps)\n",
        "    X_test_lstm, y_test_lstm = create_sequences(X_test, y_test, time_steps)\n",
        "\n",
        "    # Ensure that sequences are not empty\n",
        "    if len(X_train_lstm) == 0 or len(X_test_lstm) == 0:\n",
        "        print(\"Skipping due to insufficient data to create sequences.\")\n",
        "        continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bccb49c1",
      "metadata": {
        "id": "bccb49c1"
      },
      "outputs": [],
      "source": [
        "# Cross-validation\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "train_mse_scores = []\n",
        "test_mse_scores = []\n",
        "y_train_pred_all = []\n",
        "y_train_actual_all = []\n",
        "y_test_pred_all = []\n",
        "y_test_actual_all = []\n",
        "\n",
        "for train_index, test_index in tscv.split(X_normalized):\n",
        "    X_train, X_test = X_normalized.iloc[train_index], X_normalized.iloc[test_index]\n",
        "    y_train, y_test = y_normalized.iloc[train_index], y_normalized.iloc[test_index]\n",
        "\n",
        "    X_train_lstm, y_train_lstm = create_sequences(X_train, y_train, time_steps)\n",
        "    X_test_lstm, y_test_lstm = create_sequences(X_test, y_test, time_steps)\n",
        "\n",
        "print(\"Shape of X_train_lstm:\", X_train_lstm.shape)\n",
        "print(\"Shape of y_train_lstm:\", y_train_lstm.shape)\n",
        "print(\"Shape of X_test_lstm:\", X_test_lstm.shape)\n",
        "print(\"Shape of y_test_lstm:\", y_test_lstm.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10027b67",
      "metadata": {
        "id": "10027b67"
      },
      "outputs": [],
      "source": [
        "# Build the model using LSTM\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(units=64, return_sequences=False, kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(units=32, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dense(units=1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9413d949",
      "metadata": {
        "id": "9413d949"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model.fit(X_train_lstm, y_train_lstm, epochs=100, batch_size=16, validation_split=0.2, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5480ced",
      "metadata": {
        "id": "c5480ced"
      },
      "outputs": [],
      "source": [
        " # Predictions\n",
        "y_train_pred = model.predict(X_train_lstm)\n",
        "y_pred = model.predict(X_test_lstm)\n",
        "\n",
        "    # Inverse transform the predictions\n",
        "y_train_pred_inv = target_scaler.inverse_transform(y_train_pred)\n",
        "y_test_pred_inv = target_scaler.inverse_transform(y_pred)\n",
        "y_train_inv = target_scaler.inverse_transform(y_train_lstm)\n",
        "y_test_inv = target_scaler.inverse_transform(y_test_lstm.reshape(-1, 1))\n",
        "\n",
        "    # Store predictions for visualization\n",
        "y_train_pred_all.append(y_train_pred_inv)\n",
        "y_train_actual_all.append(y_train_inv)\n",
        "y_test_pred_all.append(y_test_pred_inv)\n",
        "y_test_actual_all.append(y_test_inv)\n",
        "\n",
        "    # Calculate MSE\n",
        "train_mse = mean_squared_error(y_train_inv, y_train_pred_inv)\n",
        "test_mse = mean_squared_error(y_test_inv, y_test_pred_inv)\n",
        "\n",
        "train_mse_scores.append(train_mse)\n",
        "test_mse_scores.append(test_mse)\n",
        "\n",
        "# Average MSE scores\n",
        "average_train_mse = np.mean(train_mse_scores)\n",
        "average_test_mse = np.mean(test_mse_scores)\n",
        "\n",
        "print(f'Average Train MSE: {average_train_mse}')\n",
        "print(f'Average Test MSE: {average_test_mse}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca8cb338",
      "metadata": {
        "id": "ca8cb338"
      },
      "outputs": [],
      "source": [
        "# Future Predictions\n",
        "def predict_future(model, X, n_steps, time_steps):\n",
        "    future_preds = []\n",
        "    last_seq = X[-time_steps:].values  # get the last sequence from the dataset\n",
        "    n_features = X.shape[1]  # number of features in the dataset\n",
        "\n",
        "    for _ in range(n_steps):\n",
        "        pred = model.predict(last_seq.reshape(1, time_steps, n_features))\n",
        "        future_preds.append(pred[0, 0])\n",
        "\n",
        "        # Slide the window forward\n",
        "        # Create a new row with the prediction and the rest of the features from the last row\n",
        "        new_row = np.zeros((1, n_features))\n",
        "        new_row[0, 0] = pred  # Assuming the prediction should be in the first column\n",
        "        new_row[0, 1:] = last_seq[-1, 1:]  # Retain the rest of the features\n",
        "\n",
        "        last_seq = np.vstack((last_seq[1:], new_row))  # append new_row and remove the first row\n",
        "\n",
        "    return np.array(future_preds)\n",
        "\n",
        "# Assuming each dataset has the same coordinates, extract coordinates from the hotspots dataset\n",
        "coordinates = hotspots_data[['latitude', 'longitude']]\n",
        "\n",
        "# Predict future values\n",
        "n_future_steps = 9  # Number of months to predict into the future\n",
        "future_predictions = predict_future(model, X_normalized, n_future_steps, time_steps)\n",
        "\n",
        "# Inverse transform the future predictions\n",
        "future_predictions_inv = target_scaler.inverse_transform(future_predictions.reshape(-1, 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9298cc92",
      "metadata": {
        "id": "9298cc92"
      },
      "outputs": [],
      "source": [
        "# Create future dates\n",
        "last_date = data_resampled.index[-1]\n",
        "future_dates = pd.date_range(start=last_date, periods=n_future_steps + 1, freq='M')[1:]\n",
        "\n",
        "# Calculate mean and standard deviation of coordinates\n",
        "mean_lat = coordinates['latitude'].mean()\n",
        "mean_lon = coordinates['longitude'].mean()\n",
        "std_lat = coordinates['latitude'].std()\n",
        "std_lon = coordinates['longitude'].std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "374e1d5b",
      "metadata": {
        "id": "374e1d5b"
      },
      "outputs": [],
      "source": [
        "# Generate varied future coordinates\n",
        "np.random.seed(42)  # for reproducibility\n",
        "future_lats = np.random.normal(loc=mean_lat, scale=std_lat, size=n_future_steps)\n",
        "future_lons = np.random.normal(loc=mean_lon, scale=std_lon, size=n_future_steps)\n",
        "\n",
        "# Create a DataFrame for future predictions with varied geospatial data\n",
        "future_geo_predictions = pd.DataFrame({\n",
        "    'latitude': future_lats,\n",
        "    'longitude': future_lons,\n",
        "    'prediction': future_predictions_inv.flatten(),\n",
        "    'date': future_dates\n",
        "})\n",
        "\n",
        "# Create a map centered around the average latitude and longitude\n",
        "m = folium.Map(location=[mean_lat, mean_lon], zoom_start=6)\n",
        "\n",
        "# Add heat map\n",
        "heat_data = [[row['latitude'], row['longitude'], row['prediction']] for index, row in future_geo_predictions.iterrows()]\n",
        "HeatMap(heat_data).add_to(m)\n",
        "\n",
        "# Save the map as an HTML file and display it\n",
        "map_filename = 'future_predictions_heatmap.html'\n",
        "m.save(map_filename)\n",
        "\n",
        "# Display the map in Jupyter Notebook\n",
        "IFrame(map_filename, width=700, height=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9f05df",
      "metadata": {
        "id": "5b9f05df"
      },
      "outputs": [],
      "source": [
        "## WALA NANI DIYA NA PART\n",
        "\n",
        "\n",
        "# Assuming each dataset has the same coordinates, extract coordinates from the hotspots dataset\n",
        "coordinates = hotspots_data[['latitude', 'longitude']]\n",
        "\n",
        "# Create a DataFrame for future predictions with geospatial data\n",
        "future_geo_predictions = pd.DataFrame({\n",
        "    'latitude': [coordinates['latitude'].iloc[-1]] * n_future_steps,\n",
        "    'longitude': [coordinates['longitude'].iloc[-1]] * n_future_steps,\n",
        "    'prediction': future_predictions_inv.flatten(),\n",
        "    'date': future_dates\n",
        "})\n",
        "\n",
        "# Create a map centered around the average latitude and longitude\n",
        "m = folium.Map(location=[future_geo_predictions['latitude'].mean(), future_geo_predictions['longitude'].mean()], zoom_start=6)\n",
        "\n",
        "# Add heat map\n",
        "heat_data = [[row['latitude'], row['longitude'], row['prediction']] for index, row in future_geo_predictions.iterrows()]\n",
        "HeatMap(heat_data).add_to(m)\n",
        "\n",
        "# Save the map as an HTML file and display it\n",
        "map_filename = 'future_predictions_heatmap.html'\n",
        "m.save(map_filename)\n",
        "\n",
        "# Display the map in Jupyter Notebook\n",
        "IFrame(map_filename, width=700, height=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf038eb7",
      "metadata": {
        "id": "bf038eb7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bb53fb0",
      "metadata": {
        "id": "0bb53fb0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8d431d8",
      "metadata": {
        "id": "f8d431d8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}